<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!-- 1. Viewport tag is CRITICAL for mobile responsiveness -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>E-Waste Detector (Mobile)</title>
    
    <!-- 2. Load Tailwind CSS for easy, clean styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- 3. Load TensorFlow.js -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>

    <style>
        /* Make the app feel more "native" */
        html, body {
            height: 100%;
            overflow: hidden;
            font-family: 'Inter', sans-serif;
        }
        body {
            background-color: #000;
        }
        /* Custom style to overlay canvas on top of video */
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 10;
        }
        #webcam {
            width: 100%;
            height: 100%;
            object-fit: cover; /* Fill the whole screen */
        }
        /* Simple loader animation */
        @keyframes spin {
            to { transform: rotate(360deg); }
        }
        .loader {
            border: 4px solid rgba(255, 255, 255, 0.3);
            border-top: 4px solid #3498db;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
        }
        /* Full-screen loader overlay */
        #loader-container {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.8);
            z-index: 99;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            color: white;
        }
        /* Button style */
        #cameraBtn {
            position: absolute;
            bottom: 50px;
            left: 50%;
            transform: translateX(-50%);
            z-index: 100;
        }
    </style>
</head>
<body class="text-white">

    <!-- Loading Indicator -->
    <div id="loader-container">
        <div class="loader"></div>
        <p id="loader-text" class="mt-4 text-lg">Loading model...</p>
    </div>

    <!-- Main Content -->
    <div id="main-content" class="relative w-full h-full">
        <!-- 
          4. 'playsinline' is CRITICAL for iOS to prevent fullscreen.
        -->
        <video id="webcam" autoplay playsinline muted class="w-full h-full"></video>
        <canvas id="canvas"></canvas>
    </div>

    <!-- Button to start camera -->
    <button id="cameraBtn" class="px-6 py-4 bg-blue-600 font-semibold rounded-lg shadow-lg hover:bg-blue-700 transition duration-300 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-opacity-50">
        Start Camera
    </button>
    <p id="status" class="hidden"></p> <!-- For debugging -->


    <script>
        // Get references to HTML elements
        const video = document.getElementById('webcam');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const statusText = document.getElementById('status');
        const loaderContainer = document.getElementById('loader-container');
        const loaderText = document.getElementById('loader-text');
        const mainContent = document.getElementById('main-content');
        const cameraBtn = document.getElementById('cameraBtn');

        let model = null;
        
        // 5. Define your class names and colors
        //    (Make sure this order matches your model's training)
        const classNames = ['Battery', 'Cable', 'PCB'];
        const colors = ['#FF3838', '#FF9D97', '#FF701F']; // Red, Pink, Orange

        // 6. Define Model Parameters
        const modelWidth = 640;
        const modelHeight = 640;
        const scoreThreshold = 0.3; // Confidence threshold
        const iouThreshold = 0.5;   // Non-Max Suppression (NMS) threshold

        /**
         * Loads the trained TF.js model.
         */
        async function loadModel() {
            try {
                // 7. Set the path to your model.json file
                //    This assumes your 'best_web_model' folder is in the same directory
                //    as this HTML file.
                const modelPath = './best_web_model/model.json';
                model = await tf.loadGraphModel(modelPath);

                // Warm up the model
                tf.tidy(() => {
                    const dummyInput = tf.zeros([1, modelWidth, modelHeight, 3]).toFloat();
                    model.predict(dummyInput);
                });

                console.log('Model loaded successfully.');
                loaderContainer.classList.add('hidden'); // Hide loader
                
            } catch (err) {
                console.error('Failed to load model:', err);
                loaderText.innerText = 'Error loading model. See console.';
            }
        }

        /**
         * Starts the webcam feed.
         */
        async function startWebcam() {
            try {
                // 8. This is the key for mobile:
                //    'facingMode: "environment"' asks for the phone's BACK camera.
                const constraints = {
                    video: {
                        facingMode: 'environment',
                        width: { ideal: 1280 }, // Request higher res
                        height: { ideal: 720 }
                    },
                    audio: false
                };
                
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                
                cameraBtn.classList.add('hidden'); // Hide button
                statusText.innerText = 'Camera started. Detecting...';

                video.onloadedmetadata = () => {
                    // Set canvas size to match video element's display size
                    canvas.width = video.clientWidth;
                    canvas.height = video.clientHeight;
                    console.log(`Canvas size set to: ${canvas.width}x${canvas.height}`);
                    
                    // Start the detection loop
                    detectFrame();
                };

            } catch (err)
 {
                console.error('Failed to start camera:', err);
                statusText.innerText = 'Could not start camera. Please grant permissions.';
                cameraBtn.classList.remove('hidden'); // Show button again
            }
        }

        /**
         * Main detection loop.
         */
        async function detectFrame() {
            if (!model) {
                requestAnimationFrame(detectFrame); // Wait for model
                return;
            }

            tf.engine().startScope(); // Start memory scope

            try {
                // 1. Pre-process the video frame
                const inputTensor = tf.browser.fromPixels(video)
                    .resizeBilinear([modelWidth, modelHeight])
                    .toFloat()
                    .div(255.0)  // Normalize to [0, 1]
                    .expandDims(0); // Add batch dimension

                // 2. Run detection
                //    YOLOv8 TF.js graph model has one output
                //    Shape: [1, num_classes + 4, 8400]
                const outputTensor = await model.executeAsync(inputTensor);
                
                // 3. Process the output
                //    This is the "missing logic" from Step 2 of the README.
                const [boxes, scores, classIndices] = processOutput(outputTensor);
                
                // 4. Run Non-Maximum Suppression (NMS)
                const selectedIndices = await tf.image.nonMaxSuppressionAsync(
                    boxes,          // [num_boxes, 4]
                    scores,         // [num_boxes]
                    10,             // max_output_size
                    iouThreshold,   // iou_threshold
                    scoreThreshold  // score_threshold
                );

                const selectedBoxes = boxes.gather(selectedIndices);
                const selectedScores = scores.gather(selectedIndices);
                const selectedClasses = classIndices.gather(selectedIndices);

                // 5. Draw the final boxes
                drawBoxes(selectedBoxes, selectedScores, selectedClasses);

                // 6. Cleanup tensors
                tf.dispose([inputTensor, outputTensor, boxes, scores, classIndices, selectedIndices, selectedBoxes, selectedScores, selectedClasses]);

            } catch (err) {
                console.error('Error during detection:', err);
            }

            requestAnimationFrame(detectFrame); // Loop
            tf.engine().endScope(); // End memory scope
        }

        /**
         * Processes the raw YOLOv8 output tensor.
         * Output shape: [1, 7, 8400] (for 3 classes)
         */
        function processOutput(outputTensor) {
            // Transpose the tensor from [1, 7, 8400] to [1, 8400, 7]
            const transposed = outputTensor.transpose([0, 2, 1]); // [1, 8400, 7]
            const data = transposed.squeeze(0); // [8400, 7]

            // Split into boxes [8400, 4] and scores [8400, 3]
            const boxes = data.slice([0, 0], [-1, 4]);
            const classProbs = data.slice([0, 4], [-1, -1]); // All class probabilities

            // Get the max score and class index for each box
            const maxScores = classProbs.max(1); // [8400]
            const classIndices = classProbs.argMax(1); // [8400]

            // Convert [cx, cy, w, h] boxes to [y1, x1, y2, x2]
            // This is required for tf.image.nonMaxSuppressionAsync
            const [cx, cy, w, h] = boxes.split([1, 1, 1, 1], 1); // Split columns
            const x1 = cx.sub(w.div(2));
            const y1 = cy.sub(h.div(2));
            const x2 = cx.add(w.div(2));
            const y2 = cy.add(h.div(2));
            
            // Re-stack as [y1, x1, y2, x2]
            const finalBoxes = tf.concat([y1, x1, y2, y2], 1);

            return [finalBoxes, maxScores, classIndices];
        }

        /**
         * Draws the bounding boxes on the canvas.
         */
        async function drawBoxes(boxes, scores, classes) {
            // Clear the canvas
            ctx.clearRect(0, 0, canvas.width, canvas.height);

            const boxesData = await boxes.array();
            const scoresData = await scores.array();
            const classesData = await classes.array();

            for (let i = 0; i < boxesData.length; i++) {
                const [ymin, xmin, ymax, xmax] = boxesData[i];
                
                // 9. IMPORTANT: Scale the boxes
                //    Model predicts on 640x640, but canvas matches the video.
                const x = xmin * canvas.width;
                const y = ymin * canvas.height;
                const width = (xmax - xmin) * canvas.width;
                const height = (ymax - ymin) * canvas.height;

                const classId = classesData[i];
                const score = scoresData[i].toFixed(2);
                const label = `${classNames[classId]}: ${score}`;
                const color = colors[classId];
                
                // Draw rectangle
                ctx.strokeStyle = color;
                ctx.lineWidth = 3;
                ctx.strokeRect(x, y, width, height);
                
                // Draw label background
                ctx.fillStyle = color;
                const textWidth = ctx.measureText(label).width;
                ctx.fillRect(x, y - 20, textWidth + 10, 20);
                
                // Draw label text
                ctx.fillStyle = '#000000'; // Black text
                ctx.font = '16px Arial';
                ctx.fillText(label, x + 5, y - 5);
            }
        }

        // Add event listeners
        window.addEventListener('load', loadModel);
        cameraBtn.addEventListener('click', startWebcam);

        // Adjust canvas on resize (e.g., phone rotation)
        window.addEventListener('resize', () => {
            if (video.srcObject) { // Only if camera is active
                canvas.width = video.clientWidth;
                canvas.height = video.clientHeight;
                console.log(`Resized canvas to: ${canvas.width}x${canvas.height}`);
            }
        });
    </script>
</body>
</html>
